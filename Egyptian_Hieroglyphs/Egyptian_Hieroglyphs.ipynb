{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq0r2RMXhh9X02j9Tsq/MF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asheta66/Machine-Learning-2024/blob/main/Egyptian_Hieroglyphs/Egyptian_Hieroglyphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "FQePk-3PxZIK",
        "outputId": "bab5ea8f-d381-4e67-e9f6-093138eacb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Searching for annotation CSVs...\n",
            "üìÇ Falling back: inferring labels from directory structure.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Could not infer (image,label) pairs from directories.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-395734281.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdf_all\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìÇ Falling back: inferring labels from directory structure.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0mdf_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_from_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;31m# Keep only existing image files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-395734281.py\u001b[0m in \u001b[0;36mdataframe_from_dirs\u001b[0;34m(base_path)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not infer (image,label) pairs from directories.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Could not infer (image,label) pairs from directories."
          ]
        }
      ],
      "source": [
        "# ===========================\n",
        "# Egyptian Hieroglyphs CNN: end-to-end (NO validation split)\n",
        "# - Auto-detect annotations CSVs and images\n",
        "# - Train/test split only\n",
        "# - Simple CNN\n",
        "# - Metrics: Accuracy, Precision, Recall, F1 (macro)\n",
        "# - Plots: Training curves, Confusion Matrices (train/test), ROC curves (train/test)\n",
        "# ===========================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,\n",
        "                             accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_curve, auc)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "BASE_PATH = \"/kaggle/input/egyptian-hieroglyphs\"\n",
        "IMG_SIZE = 128      # 64 or 128 are fine for a starter model\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15         # start simple; adjust as needed\n",
        "SEED = 42\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# Utility helpers\n",
        "# ---------------------------\n",
        "\n",
        "def list_files(patterns):\n",
        "    out = []\n",
        "    for p in patterns:\n",
        "        out.extend(glob.glob(p, recursive=True))\n",
        "    return out\n",
        "\n",
        "def find_annotation_csvs(base_path):\n",
        "    \"\"\"\n",
        "    Search for likely annotation CSVs in root and subfolders.\n",
        "    We prioritize names containing 'train', 'valid/val', 'test'. If not found,\n",
        "    we will still try to parse any '*annotation*.csv' files we see.\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "    for root, _, files in os.walk(base_path):\n",
        "        for f in files:\n",
        "            if f.lower().endswith(\".csv\") and \"annot\" in f.lower():\n",
        "                candidates.append(os.path.join(root, f))\n",
        "\n",
        "    # Also some datasets use train_annotations.csv / valid_annotations.csv etc.\n",
        "    # Add those if present.\n",
        "    for f in (\"train_annotations.csv\", \"valid_annotations.csv\", \"val_annotations.csv\", \"test_annotations.csv\"):\n",
        "        fp = os.path.join(base_path, f)\n",
        "        if os.path.isfile(fp) and fp not in candidates:\n",
        "            candidates.append(fp)\n",
        "\n",
        "    # As a last resort, some exports use exactly \"_annotations.csv\" under split folders\n",
        "    for split in (\"train\", \"valid\", \"val\", \"test\"):\n",
        "        fp = os.path.join(base_path, split, \"_annotations.csv\")\n",
        "        if os.path.isfile(fp) and fp not in candidates:\n",
        "            candidates.append(fp)\n",
        "\n",
        "    return sorted(set(candidates))\n",
        "\n",
        "def parse_annotations_csv(path):\n",
        "    \"\"\"\n",
        "    Attempt to parse a CSV into a (filepath, label) DataFrame for classification.\n",
        "    Supports common schema variants:\n",
        "      - columns like ['file','filename','image','image_path','path']\n",
        "      - label columns like ['label','class','category','name']\n",
        "    If the CSV looks like detection/segmentation (multiple rows per image),\n",
        "    we take the first class occurrence per image for a single-label classification baseline.\n",
        "    \"\"\"\n",
        "    df_raw = pd.read_csv(path)\n",
        "    # find image path column\n",
        "    cand_img_cols = [c for c in df_raw.columns if c.lower() in\n",
        "                     [\"file\", \"filename\", \"image\", \"image_path\", \"path\", \"img_path\", \"imagefile\"]]\n",
        "    if not cand_img_cols:\n",
        "        # try to guess by file-ish content\n",
        "        for c in df_raw.columns:\n",
        "            if df_raw[c].astype(str).str.contains(r\"\\.(jpg|jpeg|png|bmp|gif)$\", case=False, na=False).any():\n",
        "                cand_img_cols.append(c)\n",
        "                break\n",
        "    if not cand_img_cols:\n",
        "        raise ValueError(f\"Could not find an image path column in {path}\")\n",
        "    img_col = cand_img_cols[0]\n",
        "\n",
        "    # find label column\n",
        "    cand_label_cols = [c for c in df_raw.columns if c.lower() in\n",
        "                       [\"label\", \"class\", \"category\", \"name\"]]\n",
        "    if not cand_label_cols:\n",
        "        # some annotation formats store 'class' in another column name; try to heuristically detect\n",
        "        # e.g., 'class_name', 'category_name'\n",
        "        for c in df_raw.columns:\n",
        "            if \"class\" in c.lower() or \"label\" in c.lower() or \"category\" in c.lower():\n",
        "                cand_label_cols.append(c)\n",
        "                break\n",
        "    if not cand_label_cols:\n",
        "        raise ValueError(f\"Could not find a label/class column in {path}\")\n",
        "    label_col = cand_label_cols[0]\n",
        "\n",
        "    df = df_raw[[img_col, label_col]].copy()\n",
        "    df.columns = [\"image\", \"label\"]\n",
        "\n",
        "    # If duplicated rows per image (e.g., detection), reduce to single label per image (first)\n",
        "    df = df.groupby(\"image\", as_index=False).first()\n",
        "\n",
        "    return df\n",
        "\n",
        "def resolve_image_path(base_path, rel_or_abs):\n",
        "    \"\"\"\n",
        "    Turn an annotation 'image' entry into an actual file on disk.\n",
        "    - If absolute and exists, return as-is.\n",
        "    - If relative, try base_path/<that>, base_path/train/<that>, base_path/valid/<that>, base_path/test/<that>\n",
        "    - Also try common 'images' subfolder insertions.\n",
        "    \"\"\"\n",
        "    p = str(rel_or_abs)\n",
        "    if os.path.isabs(p) and os.path.isfile(p):\n",
        "        return p\n",
        "\n",
        "    # direct join\n",
        "    cand = os.path.join(base_path, p)\n",
        "    if os.path.isfile(cand):\n",
        "        return cand\n",
        "\n",
        "    # Try common split folders and images subfolders\n",
        "    trials = []\n",
        "    for split in (\"train\", \"valid\", \"val\", \"test\"):\n",
        "        trials.append(os.path.join(base_path, split, p))\n",
        "        trials.append(os.path.join(base_path, split, \"images\", p))\n",
        "\n",
        "    # Sometimes p already contains 'train/images/...'\n",
        "    trials.append(os.path.join(base_path, p.lstrip(\"/\")))\n",
        "\n",
        "    for t in trials:\n",
        "        if os.path.isfile(t):\n",
        "            return t\n",
        "\n",
        "    # As a fallback, if p is a basename, search for it anywhere under base_path\n",
        "    base = os.path.basename(p)\n",
        "    hits = list_files([os.path.join(base_path, \"**\", base)])\n",
        "    if hits:\n",
        "        return hits[0]\n",
        "\n",
        "    return None\n",
        "\n",
        "def dataframe_from_annotations(base_path, csv_paths):\n",
        "    frames = []\n",
        "    for csvp in csv_paths:\n",
        "        try:\n",
        "            df = parse_annotations_csv(csvp)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {csvp}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Resolve image paths\n",
        "        df[\"image\"] = df[\"image\"].apply(lambda s: resolve_image_path(base_path, s))\n",
        "        before = len(df)\n",
        "        df = df[df[\"image\"].notnull() & df[\"image\"].apply(os.path.isfile)]\n",
        "        dropped = before - len(df)\n",
        "        if dropped > 0:\n",
        "            print(f\"[{os.path.basename(csvp)}] Dropped {dropped} rows with unresolved image paths.\")\n",
        "        frames.append(df)\n",
        "\n",
        "    if not frames:\n",
        "        raise RuntimeError(\"No valid annotation CSVs could be parsed.\")\n",
        "\n",
        "    df_all = pd.concat(frames, ignore_index=True).drop_duplicates(subset=[\"image\"])\n",
        "    return df_all\n",
        "\n",
        "def dataframe_from_dirs(base_path):\n",
        "    \"\"\"\n",
        "    Fallback: infer (image, label) pairs from directory structure:\n",
        "      base_path/{train,valid,val,test,images}/*class*/image.*\n",
        "    If no split dirs, will scan all subdirs for class folders.\n",
        "    \"\"\"\n",
        "    patterns = []\n",
        "    for split in (\"train\", \"valid\", \"val\", \"test\"):\n",
        "        # Common: base_path/split/<class>/*.jpg\n",
        "        patterns.append(os.path.join(base_path, split, \"*\", \"*.*\"))\n",
        "        # Alternate: base_path/split/images/<class>/*.jpg\n",
        "        patterns.append(os.path.join(base_path, split, \"images\", \"*\", \"*.*\"))\n",
        "    # Global fallback: base_path/*class*/*.*\n",
        "    patterns.append(os.path.join(base_path, \"*\", \"*.*\"))\n",
        "    paths = list_files(patterns)\n",
        "\n",
        "    rows = []\n",
        "    for p in paths:\n",
        "        if re.search(r\"\\.(jpg|jpeg|png|bmp|gif)$\", p, flags=re.I):\n",
        "            # label = parent folder name (one above), but if 'images' present, take the folder above it\n",
        "            parts = os.path.normpath(p).split(os.sep)\n",
        "            if \"images\" in parts:\n",
        "                idx = parts.index(\"images\")\n",
        "                if idx + 2 <= len(parts) - 1:\n",
        "                    label = parts[idx + 1]\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                # class folder is parent directory\n",
        "                label = parts[-2]\n",
        "            rows.append((p, label))\n",
        "    if not rows:\n",
        "        raise RuntimeError(\"Could not infer (image,label) pairs from directories.\")\n",
        "    df = pd.DataFrame(rows, columns=[\"image\", \"label\"]).drop_duplicates(subset=[\"image\"])\n",
        "    return df\n",
        "\n",
        "def load_image_tf(path, img_size=IMG_SIZE):\n",
        "    img = tf.io.read_file(path)\n",
        "    # Try decode: default 3 channels RGB\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)  # to [0,1]\n",
        "    img = tf.image.resize(img, (img_size, img_size), antialias=True)\n",
        "    return img\n",
        "\n",
        "def make_tf_dataset(paths, labels, batch_size=BATCH_SIZE, shuffle=False):\n",
        "    ds_paths = tf.data.Dataset.from_tensor_slices(np.array(paths))\n",
        "    ds_imgs = ds_paths.map(lambda p: load_image_tf(p), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds_labels = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    ds = tf.data.Dataset.zip((ds_imgs, ds_labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(paths), seed=SEED, reshuffle_each_iteration=True)\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "def build_simple_cnn(num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', padding=\"same\", input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        layers.MaxPooling2D(2),\n",
        "\n",
        "        layers.Conv2D(64, (3,3), activation='relu', padding=\"same\"),\n",
        "        layers.MaxPooling2D(2),\n",
        "\n",
        "        layers.Conv2D(128, (3,3), activation='relu', padding=\"same\"),\n",
        "        layers.MaxPooling2D(2),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def plot_training_curves(history):\n",
        "    fig1 = plt.figure(figsize=(6,4))\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.title('Training Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fig2 = plt.figure(figsize=(6,4))\n",
        "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "    plt.title('Training Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_and_report(name, y_true, y_prob, class_names):\n",
        "    \"\"\"\n",
        "    Compute metrics and plot confusion matrix + ROC curves.\n",
        "    Returns dict of metrics.\n",
        "    \"\"\"\n",
        "    y_pred = np.argmax(y_prob, axis=1)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\n=== {name} Metrics ===\")\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f} (macro)\")\n",
        "    print(f\"Recall   : {rec:.4f} (macro)\")\n",
        "    print(f\"F1       : {f1:.4f} (macro)\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=range(len(class_names)))\n",
        "    fig_cm, ax = plt.subplots(1, 1, figsize=(5.5, 4.5))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(ax=ax, colorbar=False, cmap=\"Greens\")\n",
        "    ax.set_title(f\"{name} Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curves (one-vs-rest)\n",
        "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
        "    # Some classes may not appear in split; guard against degenerate curves\n",
        "    fpr = dict(); tpr = dict(); roc_auc = dict()\n",
        "    for i in range(len(class_names)):\n",
        "        if y_true_bin[:, i].max() == 0:\n",
        "            # class i not present in ground truth -> skip\n",
        "            continue\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # micro-average\n",
        "    if y_true_bin.sum() > 0:\n",
        "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())\n",
        "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # macro-average (average of AUCs over classes that exist)\n",
        "    valid_aucs = [roc_auc[i] for i in roc_auc.keys() if isinstance(i, int)]\n",
        "    macro_auc = np.mean(valid_aucs) if valid_aucs else np.nan\n",
        "\n",
        "    fig_roc = plt.figure(figsize=(6,5))\n",
        "    # plot each class curve (only those with positives)\n",
        "    for i in range(len(class_names)):\n",
        "        if i in roc_auc:\n",
        "            plt.plot(fpr[i], tpr[i], label=f\"Class {class_names[i]} (AUC={roc_auc[i]:.3f})\")\n",
        "    # optional micro\n",
        "    if \"micro\" in roc_auc:\n",
        "        plt.plot(fpr[\"micro\"], tpr[\"micro\"], linestyle=\"--\", label=f\"Micro-average (AUC={roc_auc['micro']:.3f})\")\n",
        "\n",
        "    plt.plot([0,1], [0,1], linestyle=':', label='Chance')\n",
        "    plt.title(f\"{name} ROC Curves (macro AUC={macro_auc:.3f})\")\n",
        "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
        "    plt.legend(fontsize=8, loc=\"lower right\")\n",
        "    plt.grid(True, linestyle=\":\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"macro_auc\": macro_auc}\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Discover annotations and build (image,label) table\n",
        "# ---------------------------\n",
        "print(\"üîé Searching for annotation CSVs...\")\n",
        "csvs = find_annotation_csvs(BASE_PATH)\n",
        "for c in csvs:\n",
        "    print(\" -\", c)\n",
        "\n",
        "df_all = None\n",
        "if csvs:\n",
        "    try:\n",
        "        df_all = dataframe_from_annotations(BASE_PATH, csvs)\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Failed to parse annotation CSVs robustly. Falling back to folder-based discovery. Reason:\", e)\n",
        "\n",
        "if df_all is None:\n",
        "    print(\"üìÇ Falling back: inferring labels from directory structure.\")\n",
        "    df_all = dataframe_from_dirs(BASE_PATH)\n",
        "\n",
        "# Keep only existing image files\n",
        "df_all = df_all[df_all[\"image\"].apply(lambda p: isinstance(p, str) and os.path.isfile(p))].copy()\n",
        "df_all = df_all.drop_duplicates(subset=[\"image\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nTotal unique images found: {len(df_all)}\")\n",
        "print(\"Example rows:\")\n",
        "display(df_all.sample(min(5, len(df_all)), random_state=SEED))\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Encode labels and split into train/test (NO validation)\n",
        "# ---------------------------\n",
        "le = LabelEncoder()\n",
        "df_all[\"label_enc\"] = le.fit_transform(df_all[\"label\"].astype(str))\n",
        "class_names = list(le.classes_)\n",
        "num_classes = len(class_names)\n",
        "print(f\"\\nClasses ({num_classes}): {class_names}\")\n",
        "\n",
        "# Stratified split\n",
        "train_df, test_df = train_test_split(\n",
        "    df_all[[\"image\", \"label_enc\"]],\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=df_all[\"label_enc\"]\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain size: {len(train_df)}\")\n",
        "print(f\"Test  size: {len(test_df)}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Build tf.data pipelines\n",
        "# ---------------------------\n",
        "train_paths = train_df[\"image\"].tolist()\n",
        "train_labels = train_df[\"label_enc\"].values\n",
        "\n",
        "test_paths = test_df[\"image\"].tolist()\n",
        "test_labels = test_df[\"label_enc\"].values\n",
        "\n",
        "train_ds = make_tf_dataset(train_paths, train_labels, shuffle=True)\n",
        "test_ds  = make_tf_dataset(test_paths,  test_labels,  shuffle=False)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Build and train model (NO validation)\n",
        "# ---------------------------\n",
        "model = build_simple_cnn(num_classes)\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Training curves (loss + accuracy)\n",
        "plot_training_curves(history)\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Evaluate on Train and Test (metrics + confusion matrices + ROC)\n",
        "# ---------------------------\n",
        "# Get probabilities\n",
        "train_probs = model.predict(train_ds, verbose=0)\n",
        "test_probs  = model.predict(test_ds,  verbose=0)\n",
        "\n",
        "# True labels for batches (preserve original order)\n",
        "y_train_true = np.concatenate([y.numpy() for _, y in train_ds.unbatch()])\n",
        "y_test_true  = np.concatenate([y.numpy() for _, y in test_ds.unbatch()])\n",
        "\n",
        "# Metrics + Plots for Train\n",
        "train_metrics = evaluate_and_report(\"Train\", y_train_true, train_probs, class_names)\n",
        "\n",
        "# Metrics + Plots for Test\n",
        "test_metrics = evaluate_and_report(\"Test\", y_test_true, test_probs, class_names)\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Print a compact summary table\n",
        "# ---------------------------\n",
        "summary = pd.DataFrame([\n",
        "    {\"Split\": \"Train\", **{k.capitalize(): v for k, v in train_metrics.items()}},\n",
        "    {\"Split\": \"Test\",  **{k.capitalize(): v for k, v in test_metrics.items()}}\n",
        "])\n",
        "print(\"\\n=== Summary (macro metrics) ===\")\n",
        "display(summary)\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Show a few sample predictions (optional)\n",
        "# ---------------------------\n",
        "def show_samples(paths, y_true, y_prob, class_names, k=6, title=\"Samples\"):\n",
        "    idxs = np.random.default_rng(SEED).choice(len(paths), size=min(k, len(paths)), replace=False)\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    fig.suptitle(title)\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        img = plt.imread(paths[idx])\n",
        "        pred = np.argmax(y_prob[idx])\n",
        "        tt = class_names[y_true[idx]]\n",
        "        pp = class_names[pred]\n",
        "        plt.subplot(2, (k+1)//2, i)\n",
        "        try:\n",
        "            plt.imshow(img)\n",
        "        except:\n",
        "            plt.imshow(img, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"T:{tt}\\nP:{pp}\", fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_samples(test_paths, y_test_true, test_probs, class_names, k=6, title=\"Test Samples (T=true, P=pred)\")\n"
      ]
    }
  ]
}